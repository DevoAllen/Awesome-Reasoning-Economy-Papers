# <img src="figures/productivity.png" alt="Example Figure" width="50" height="50" /> Harnessing the Inference Economy: A Survey of Efficient Reasoning for Large Language Models

[![arXiv](https://img.shields.io/badge/arXiv-Reasoning_Economy-b31b1b.svg)]()
[![Contribution Welcome](https://img.shields.io/badge/Contributions-welcome-blue)]()

<!-- omit in toc -->
## ğŸ“¢ Updates

- **2025.03**: We released a github repo to record papers related with reasoning economy. Feel free to cite or open pull requests.

<!-- omit in toc -->
## ğŸ“’ Table of Contents

- [Harnessing the Inference Economy: A Survey of Efficient Reasoning for Large Language Models](#harnessing-the-inference-economy-a-survey-of-efficient-reasoning-for-large-language-models)
    - [1. Post-training Methods for Efficient Thinking reasoning LLMs](#ï¸-1-post-training-methods-for-efficient-thinking-reasoning-llms)
      - [1.1 Post-training Induced Inefficiency](#11---post-training-induced-inefficiency)
        - [Length-bias](#length-bias)
        - [Deceptive Behaviors](#deceptive-behaviors)
      - [1.2 Mitigating Solutions](#12mitigating-solutions)
        - [Length-bias Alleviation](#length-bias-alleviation)
        - [Deceptive Behaviors Alleviation](#deceptive-behaviors-alleviation)
    - [2. Refineing Test-time Methods for Efficient Reasoning](#ï¸-2--refineing-test-time-methods-for-efficient-reasoning)
      - [2.1 Test-time Methods Induced Inefficiency](#21---test-time-methods-induced-inefficiency)
      - [2.2 (Mitigating) Solutions](#22--mitigating-solutions)
        - [2.2.1 Budget Prediction \& Allocation before Decoding](#221--budget-prediction--allocation-before-decoding)
        - [2.2.2 Â Â   Adaptive Budget Allocation During Decoding](#222---adaptive-budget-allocation-during-decoding)
    - [3. Post-training Calibrated with Inference Algorithm](#ï¸-3---post-training-calibrated-with-inference-algorithm)
    - [4. Emerging Frontiers in Efficient Reasoning](#ï¸-4---emerging-frontiers-in-efficient-reasoning)
      - [4.1 Chain-of-Thought Compression](#41--chain-of-thought-compression)
      - [4.2 System-1 and System-2 Cooperation](#42--system-1-and-system-2-cooperation)
      - [4.3 Recurrent Depth Reasoning](#43--recurrent-depth-reasoning)



### 1. Post-training Methods for Efficient Thinking reasoning LLMs

RL optimization often relies on reward models (RMs) that are inherently imperfect, primarily due to unreliable human preference annotations. Additionally, Goodhartâ€™s Law states, â€œWhen a measure becomes a target, it ceases to be a good measure.â€ Consequently, over-optimizing based on these flawed RMs can negatively impact the overall capabilities of LLMs.

This situation highlights the risk of reward hacking, where models exploit the reward function to achieve high scores without genuinely aligning with human preferences. 
As a result, LLMs may exhibit **Superficial Alignment**, appearing to meet human expectations while lacking true understanding.

#### 1.1&nbsp;&nbsp;   Post-training Induced Inefficiency

##### Length-bias
LLMs trained with RL tend to **produce longer responses** compared to those trained through SFT.

- **[A Long Way to Go: Investigating Length Correlations in RLHF](https://openreview.net/forum?id=G8LaO1P0xv#discussion)** 
- **[Fine-grained human feedback gives better rewards for language model training](https://dl.acm.org/doi/abs/10.5555/3666122.3668696)** 
- **[Learning to summarize from human feedback](https://dl.acm.org/doi/10.5555/3495724.3495977)** 


**â—ï¸&nbsp;&nbsp;Findings  of Overly Cautious / Overthinking reasoning LLMs**

Reasoning LLMs exhibit excessive unnecessary verification and redundant reasoning on easy-to-handle questions, leading to inefficient token usage and increased computational costs.

- **[When More is Less: Understanding Chain-of-Thought Length in LLMs](https://arxiv.org/abs/2502.07266)**
- **[The Danger of Overthinking: Examining the Reasoning-Action Dilemma in Agentic Tasks](https://arxiv.org/abs/2502.08235)**
- **[Do NOT Think That Much for 2+3=? On the Overthinking of o1-Like LLMs](https://arxiv.org/abs/2412.21187)**
- **[The Impact of Reasoning Step Length on Large Language Models](https://aclanthology.org/2024.findings-acl.108/)**
- **[Concise Thoughts: Impact of Output Length on LLM Reasoning and Cost](https://arxiv.org/abs/2407.19825)**



##### Deceptive Behaviors


- Fake Alignment: Are LLMs Really Aligned Well?


**â—ï¸&nbsp;&nbsp;Findings of Fake Thinking reasoning LLMs**
Deceptive Behavior is even harder to detect than length bias.
Previous research has demonstrated that LLMs may display differential behaviors across various demographic groups \cite{fake-align-1}, thereby raising concerns regarding the authenticity and fairness of their alignment with human values.

- [When Can LLMs Actually Correct Their Own Mistakes? A Critical Survey of Self-Correction of LLMs]
- 
#### 1.2&nbsp;&nbsp;Mitigating Solutions

##### Length-bias Alleviation
Disentangling the length and quality reward.

Length reward penalty (kl divergense, length penal).

Length preference optimization, (DPO, preference partial pair reconstruction).
- [SimPO: Simple Preference Optimization with a Reference-Free Reward](https://openreview.net/forum?id=3Tzcot1LKb)
- 
**âœ¨&nbsp;&nbsp;Overly Cautious / Overthinking reasoning LLMs**
Emerging research papers for reasoning LLMs

- [Self-Training Elicits Concise Reasoning in Large Language Models](https://arxiv.org/abs/2502.20122)
  - self-training for long2short
- [O1-Pruner: Length-Harmonizing Fine-Tuning for O1-Like Reasoning Pruning](https://arxiv.org/abs/2501.12570)
- 
##### Deceptive Behaviors Alleviation


**âœ¨&nbsp;&nbsp;Fake Thinking reasoning LLMs**



**âœ¨&nbsp;&nbsp; For Both of Them**

- [Training Language Models to Reason Efficiently]
  - è®­ç»ƒæ¨¡å‹æ ¹æ®éš¾åº¦åˆ†é…ç®—åŠ›
- Token-Budget-Aware LLM Reasoning


---

### 2. Refineing Test-time Methods for Efficient Reasoning

#### 2.1 Test-time Methods Induced Inefficiency

**è§£ætest-timeç®—æ³•ï¼Œæ¢ç©¶å“ªäº›å› ç´ å¯¼è‡´test-timeç®—æ³•é€ æˆè®¡ç®—èµ„æºæµªè´¹ã€‚**

sequential inference: self-refineä½•æ—¶åœæ­¢ï¼Ÿ
- [Same Task, More Tokens: the Impact of Input Length on the Reasoning Performance of Large Language Models](https://aclanthology.org/2024.acl-long.818/)

parallel inference: é‡‡æ ·çª—å£å¤ªå¤§ / å¤ªå°ï¼Œå¯¼è‡´è®¡ç®—èµ„æºæµªè´¹ï¼›
- [Are More LLM Calls All You Need? Towards Scaling Laws of Compound Inference Systems](https://proceedings.neurips.cc/paper_files/paper/2024/hash/51173cf34c5faac9796a47dc2fdd3a71-Abstract-Conference.html)
- [Cerberus: Efficient Inference with Adaptive Parallel Decoding and Sequential Knowledge Enhancement](https://arxiv.org/abs/2410.13344)

search è¿‡ç¨‹ä¸­ï¼š
- inter-sample
  1. ç®€å•é¢˜è¿‡åº¦æ¢ç´¢
  2. éš¾é¢˜æ¢ç´¢ä¸è¶³
- intra-sample
  1. æ— æ•ˆæ¢ç´¢
  2. è¿‡åº¦æ¢ç´¢
   
- [Mind Your Step (by Step): Chain-of-Thought can Reduce Performance on Tasks where Thinking Makes Humans Worse](https://arxiv.org/abs/2410.21333)
- [Enhancing LLM Reasoning with Reward-guided Tree Search](https://arxiv.org/abs/2411.11694)

#### 2.2&nbsp;&nbsp;  (Mitigating) Solutions

**é’ˆå¯¹ä»¥ä¸Šå¯¹test-timeç®—æ³•é€ æˆçš„è®¡ç®—èµ„æºæµªè´¹ï¼Œå¦‚ä½•æ”¹è¿›ï¼Ÿ**

##### 2.2.1&nbsp;&nbsp;  Budget Prediction & Allocation before Decoding

**Direct Prediction**

- [Token-Budget-Aware LLM Reasoning]()
- [s1: Simple test-time scaling]()
- [Following Length Constraints in Instructions]
- [Concise Thoughts: Impact of Output Length on LLM Reasoning and Cost]

**Difficulty-aware Prediction**
- [Make Every Penny Count: Difficulty-Adaptive Self-Consistency for Cost-Efficient Reasoning](http://arxiv.org/abs/2408.13457)




##### 2.2.2 Adaptive Budget Allocation During Decoding

éšç€æ¨ç†è¿›è¡Œï¼Œåœ¨æœç´¢è¿‡ç¨‹ä¸­è¿›è¡Œå‰ªæã€early stopç­‰æ“ä½œã€‚

**Early Stopping**
- [Adaptive Inference-Time Compute: LLMs Can Predict if They Can Do Better, Even Mid-Generation](https://arxiv.org/abs/2410.02725)
  - self-evaluationåˆ¤æ–­early stop
  
- [Escape Sky-high Cost: Early-stopping Self-Consistency for Multi-step Reasoning](http://arxiv.org/abs/2401.10480)
  - ç­”æ¡ˆçš„consistencyåˆ¤æ–­early stop

- [Letâ€™s Sample Step by Step: Adaptive-Consistency for Efficient Reasoning and Coding with LLMs](https://aclanthology.org/2023.emnlp-main.761/)


**Pruning while Searching**
- [Enhancing LLM Reasoning with Reward-guided Tree Search](https://arxiv.org/abs/2411.11694)
  - RMå‰ªæ
- [Math-Shepherd: Verify and Reinforce LLMs Step-by-step without Human Annotations](https://aclanthology.org/2024.acl-long.510/)
- [OVM, Outcome-supervised Value Models for Planning in Mathematical Reasoning](https://aclanthology.org/2024.findings-naacl.55/)


---

### 3 Post-training Calibrated with Inference Algorithm

Design post-training methods, along with corresponding inference algorithms, to achieve efficient reasoning.


- [Learning How Hard to Think: Input-Adaptive Allocation of LM Computation](http://arxiv.org/abs/2410.04707)
- [InfAlign: Inference-aware language model alignment](https://arxiv.org/abs/2412.19792)
- [BOND: Aligning LLMs with Best-of-N Distillation](https://arxiv.org/abs/2407.14622)
  - è®­ç»ƒå¯¹é½BoNï¼Œä¸éœ€è¦æµ‹è¯•æ—¶ä½¿ç”¨å®Œå…¨ç‰ˆBoNï¼Œå‡å°‘å¼€é”€
- [Adaptive Decoding via Latent Preference Optimization](https://arxiv.org/abs/2411.09661)
  - è®­ç»ƒï¼šé¢„æµ‹æ¯ä¸ªtokenéœ€è¦çš„æ¸©åº¦
  - æ¨ç†ï¼šä½¿ç”¨è®­ç»ƒåçš„æ¸©åº¦é¢„æµ‹å™¨ç¡®å®šæ¯ä¸ªtokenä½¿ç”¨çš„æ¸©åº¦
- [L1: Controlling How Long A Reasoning Model Thinks With Reinforcement Learning](https://arxiv.org/abs/2503.04697)
  - è®­ç»ƒï¼šLCPOï¼Œè®­ç»ƒä¸­ä¸¤ä¸ªå¥–åŠ±ï¼šaccuracyï¼Œå’Œæ»¡è¶³promptä¸­é•¿åº¦
  - æ¨ç†ï¼špromptæŒ‡å®šæ¨ç†é•¿åº¦


### 4. Emerging Frontiers in Efficient Reasoning


#### 4.1 Chain-of-Thought Compression

å‹ç¼©cotï¼Œç¼©çŸ­æ¨ç†é•¿åº¦ã€‚


**Explicit Compression**
- [TokenSkip: Controllable Chain-of-Thought Compression in LLMs](https://arxiv.org/abs/2502.12067.pdf)
  - è·³è¿‡ä¸å…³é”®token
- [Stepwise Perplexity-Guided Refinement for Efficient Chain-of-Thought Reasoning in Large Language Models](https://arxiv.org/abs/2502.13260.pdf)
  - è·³è¿‡ä¸å…³é”®æ­¥éª¤ï¼Œä¸“æ³¨å…³é”®æ­¥éª¤

**Implicit Compression**
- [Think before you speak: Training Language Models With Pause Tokens] `do not think this is relevant`
- [SoftCoT: Soft Chain-of-Thought for Efficient Reasoning with LLMs](https://arxiv.org/abs/2502.12134.pdf)
- [LightThinker: Thinking Step-by-Step Compression](https://arxiv.org/pdf/2502.15589.pdf)
- [Compressed chain of thought: Efficient reasoning through dense representations.](https://arxiv.org/pdf/2412.13171.pdf)
- [Training Large Language Models to Reason in a Continuous Latent Space](https://arxiv.org/pdf/2412.06769.pdf)
- [From Explicit CoT to Implicit CoT: Learning to Internalize CoT Step by Step](https://arxiv.org/abs/2405.14838.pdf)
- [Implicit Chain of Thought Reasoning via Knowledge Distillation](https://arxiv.org/abs/2311.01460.pdf)

#### 4.2&nbsp;&nbsp;  System-1 and System-2 Cooperation

å°½ç®¡proverï¼Œveriferçš„æ¨¡å‹ä¹Ÿå¯ä»¥ç§°ä½œåŒæ¨¡å‹åˆä½œï¼Œä½†æ­¤å¤„æˆ‘ä»¬ç‰¹æŒ‡åŒproverçš„åˆä½œ

**åŒæ¶æ„æ¨¡å‹åä½œ**

## Single-Model Routing

- [System-1.x: Learning to Balance Fast and Slow Planning with Language Models](https://arxiv.org/pdf/2407.14414.pdf)
- [DynaThink: Fast or Slow? A Dynamic Decision-Making Framework for Large Language Models](https://aclanthology.org/2024.emnlp-main.814/)

## Multi-Model Collaboration

### Speculative Decoding

- [JUDGE DECODING: FASTER SPECULATIVE SAMPLING REQUIRES GOING BEYOND MODEL ALIGNMENT](https://arxiv.org/pdf/2501.19309.pdf)
- [Reward-Guided Speculative Decoding for Efficient LLM Reasoning](https://arxiv.org/pdf/2501.19324.pdf)
- [MAgICoRe: Multi-Agent, Iterative, Coarse-to-Fine Refinement for Reasoning](https://arxiv.org/abs/2409.12147.pdf) `also not very suitable here`
- [Unlocking Efficiency in Large Language Model Inference: A Comprehensive Survey of Speculative Decoding](https://aclanthology.org/2024.findings-acl.456.pdf)
- [Speculative Decoding: Exploiting Speculative Execution for Accelerating Seq2seq Generation](https://aclanthology.org/2023.findings-emnlp.257/)
- [Speculative Decoding with Big Little Decoder](https://arxiv.org/pdf/2302.07863.pdf)
- [Fast Inference from Transformers via Speculative Decoding](https://arxiv.org/pdf/2211.17192.pdf)

### Multi-model Workflow

- [CITER: Collaborative Inference for Efficient Large Language Model Decoding with Token-Level Routing](https://arxiv.org/pdf/2502.01976.pdf)
- [SwiftSage: A Generative Agent with Fast and Slow Thinking for Complex Interactive Tasks](https://arxiv.org/abs/2305.17390.pdf)
- [Synergy-of-Thoughts: Eliciting Efficient Reasoning in Hybrid Language Models](https://arxiv.org/pdf/2402.02563.pdf)

  

**å¼‚æ¶æ„æ¨¡å‹åä½œ**
too few


**Distill System 2 to System 1**

åŒæ„ï¼ˆå¤§transformerè’¸é¦åˆ°å°transformerï¼‰
- [DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning](https://arxiv.org/pdf/2501.12948.pdf)
  - deepseekdistilled qwen
- [Distilling System 2 into System 1](https://arxiv.org/abs/2407.06023.pdf)

å¼‚æ„ï¼ˆé«˜å¤æ‚åº¦çš„transformerï¼Œè’¸é¦åˆ°ä½å¤æ‚åº¦æ¨¡å‹mambaï¼‰
transformerè’¸é¦åˆ°mamba
- [Thinking Slow, Fast: Scaling Inference Compute with Distilled Reasoners](https://arxiv.org/abs/2502.20339.pdf)

#### 4.3 Recurrent Depth Reasoning
1. æ¨ç†ä¸­æ¨¡å‹æ·±åº¦å¾ˆé‡è¦
   - [Physics of Language Models: Part 2.1]
     - Language model depth is crucial for mathematical reasoning.
   - [What Can Transformer Learn with Varying Depth? Case Studies on Sequence Learning Tasks]
     - æ¨¡å‹ä¸åŒæ·±åº¦å­¦ä¹ èƒ½åŠ›ä¸åŒï¼Œå³ä½¿å‚æ•°ç›¸åŒ
2. å°æ¨¡å‹é‡å¤ä½¿ç”¨å…¶ä¸­æŸäº›å±‚ï¼Œå®ç°æ·±åº¦æ¨ç†
   - [Scaling up Test-Time Compute with Latent Reasoning: A Recurrent Depth Approach]
   - [Enhancing Auto-regressive Chain-of-Thought through Loop-Aligned Reasoning]

3. è¿™æ ·è®­ç»ƒã€æ¨ç†å¼€é”€éƒ½å°
4. ä¹Ÿå¯ä»¥è·³è¿‡transformeræŸäº›ä¸é‡è¦çš„å±‚
   - [Not All Layers of LLMs Are Necessary During Inference](https://arxiv.org/abs/2403.02181)


---


## Citation
If you find this work useful, welcome to cite us.
```bib

```

<!-- omit in toc -->
## â­ Star History

<a href="https://www.star-history.com/#DevoAllen/EfficientThinking&Date">
 <picture>
   <source media="(prefers-color-scheme: dark)" srcset="https://api.star-history.com/svg?repos=DevoAllen/EfficientThinking&type=Date&theme=dark" />
   <source media="(prefers-color-scheme: light)" srcset="https://api.star-history.com/svg?repos=DevoAllen/EfficientThinking&type=Date&type=Date" />
   <img alt="Star History Chart" src="https://api.star-history.com/svg?repos=DevoAllen/EfficientThinking&type=Date&type=Date" />
 </picture>
</a>



